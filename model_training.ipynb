{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWd-QslcaVmm"
      },
      "outputs": [],
      "source": [
        "!pip install medigan\n",
        "!pip install torchxrayvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#!pip install medigan\n",
        "#!pip install torchxrayvision\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from medigan import Generators\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import torchxrayvision as xrv\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import os\n",
        "from drive.MyDrive.ECE661_Project.PGGAN_CHEST_XRAY.model20.network import ProGenerator, ProDiscriminator, ProGAN\n",
        "from drive.MyDrive.ECE661_Project.PGGAN_CHEST_XRAY.model20.configuration import hparams\n",
        "import torchvision.transforms as transforms\n",
        "import torch.autograd as autograd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "itqF05rhafPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install dependencies and load pretrained PGGAN\n",
        "gens = Generators()\n",
        "dataloader = gens.get_as_torch_dataloader(\n",
        "    model_id=20,\n",
        "    num_samples=1,\n",
        "    install_dependencies=True,\n",
        "    prefetch_factor=None\n",
        ")\n",
        "\n",
        "from drive.MyDrive.ECE661_Project.PGGAN_CHEST_XRAY.model20.network import ProGenerator, ProDiscriminator, ProGAN\n",
        "from drive.MyDrive.ECE661_Project.PGGAN_CHEST_XRAY.model20.configuration import hparams"
      ],
      "metadata": {
        "id": "5c25mTuOajo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XrayDataset(Dataset):\n",
        "    def __init__(self, images_raw, labels, transform=None):\n",
        "        images = torch.tensor(images_raw, dtype=torch.float32).unsqueeze(1)  #[N,1,H,W]\n",
        "        if images.max() > 1.0:\n",
        "            images = images.div(255.0)\n",
        "        self.images = images\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)  #[N,3]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]    # [1, H, W]\n",
        "        lbl = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, lbl\n",
        "\n",
        "class GANDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        base = xrv.models.DenseNet(weights=\"densenet121-res224-chex\")  # pretrained DenseNet121\n",
        "        # freeze feature extractor layers\n",
        "        for p in base.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # extract feature and pool\n",
        "        self.features = base.features\n",
        "        self.pool = base.pool if hasattr(base, 'pool') else nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "        # binary head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.features(x)\n",
        "        f = self.pool(f)\n",
        "        f = f.flatten(1)\n",
        "        return self.head(f)"
      ],
      "metadata": {
        "id": "2Gz8bBI7a-_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pgan = (ProGAN(hparams).load_model( \"models/00020_PGGAN_CHEST_XRAY/Final_Full_Model.pth\", map_location='cpu', image_size=1024))\n",
        "generator = pgan.generator.to(device)\n",
        "\n",
        "for name, param in generator.named_parameters(): # Freeze all except last deconv block\n",
        "    param.requires_grad = False\n",
        "\n",
        "for name, param in generator.named_parameters():\n",
        "    if name.startswith(\"ScaleBlocks.6.\") or name.startswith(\"ScaleBlocks.7.\") or name.startswith(\"toRGB.7.\") or name.startswith(\"toRGB.8.\"):\n",
        "        param.requires_grad = True\n",
        "for name, param in generator.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.requires_grad)\n",
        "\n",
        "discriminator = GANDiscriminator().to(device)\n",
        "\n",
        "gen_optimizer = optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()),\n",
        "                           lr=1e-4, betas=(0.5,0.9))\n",
        "\n",
        "disc_optimizer = optim.Adam(discriminator.parameters(), lr=2e-5, betas=(0.5,0.9))"
      ],
      "metadata": {
        "id": "kKh4YAdIata2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_gray_full = np.load(\"/content/drive/MyDrive/ECE661_Project/CheXpert-v1.0-small/train_image.npy\")\n",
        "print(images_gray_full.shape)\n",
        "\n",
        "images_gray = images_gray_full#[:5000]\n",
        "labels = labels_full#[:5000]\n",
        "\n",
        "\n",
        "XRAY_MEAN_1CH, XRAY_STD_1CH = [0.502], [0.290]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224), antialias=True),\n",
        "    # Normalize input in [0, 1] to [-1, 1]\n",
        "    transforms.Normalize(mean=XRAY_MEAN_1CH, std=XRAY_STD_1CH)\n",
        "])\n",
        "\n",
        "b_size = 32\n",
        "\n",
        "dataset = XrayDataset(images_gray, labels, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=b_size, shuffle=True)"
      ],
      "metadata": {
        "id": "o22SR0HMcLdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_gp = 20\n",
        "d_steps = 2\n",
        "\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "weights = torch.tensor([0.2989, 0.5870, 0.1140], device=device).view(1, 3, 1, 1) # Not needed\n",
        "iter = 0\n",
        "epoch = 30\n",
        "target_size = (224, 224)\n",
        "\n",
        "print(f\"Starting Training: d_steps={d_steps}, lambda_gp={lambda_gp}\")\n",
        "\n",
        "for e in range(epoch):\n",
        "    for real_imgs_normalized, real_lbls in dataloader:\n",
        "        real_imgs_normalized = real_imgs_normalized.to(device)\n",
        "        bs = real_imgs_normalized.size(0)\n",
        "\n",
        "        for _ in range(d_steps): #critic\n",
        "            disc_optimizer.zero_grad()\n",
        "\n",
        "            # Generate fake image batch\n",
        "            z = torch.randn(bs, 512, device=device)\n",
        "            fake_imgs = generator(z)\n",
        "            fake_imgs = torch.tanh(fake_imgs)\n",
        "            fake_imgs_resized = F.interpolate(fake_imgs, size=target_size, mode='bilinear', align_corners=False) #Resize\n",
        "            fake_gray = (fake_imgs_resized * weights).sum(dim=1, keepdim=True) #grayscale\n",
        "            fake_gray_normalized = torch.tanh(fake_gray) #normalize\n",
        "\n",
        "            # Real images\n",
        "            real_logits = discriminator(real_imgs_normalized)\n",
        "            fake_logits = discriminator(fake_gray_normalized.detach())\n",
        "            gradient_penalty = compute_gradient_penalty(discriminator, real_imgs_normalized.data, fake_gray_normalized.data, device)\n",
        "\n",
        "            d_loss = torch.mean(fake_logits) - torch.mean(real_logits) + lambda_gp * gradient_penalty #d_loss\n",
        "\n",
        "            d_loss.backward()\n",
        "            disc_optimizer.step()\n",
        "\n",
        "\n",
        "        gen_optimizer.zero_grad()\n",
        "\n",
        "        z = torch.randn(bs, 512, device=device)\n",
        "        fake_imgs = generator(z)\n",
        "        fake_imgs = torch.tanh(fake_imgs)\n",
        "        fake_imgs_resized = F.interpolate(fake_imgs, size=target_size, mode='bilinear', align_corners=False) #resize\n",
        "        fake_gray =(fake_imgs_resized * weights).sum(dim=1, keepdim=True) #grayscale\n",
        "        fake_gray_normalized = torch.tanh(fake_gray) #normalize\n",
        "\n",
        "        fake_logits = discriminator(fake_gray_normalized)\n",
        "        g_loss = -torch.mean(fake_logits) #g_loss\n",
        "\n",
        "        g_loss.backward()\n",
        "        gen_optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "        if iter % 300 == 0:\n",
        "\n",
        "             print(f\"Iter {iter}: d_loss={d_loss.item():.4f}, g_loss={g_loss.item():.4f}, gp={gradient_penalty.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch {e}: d_loss={d_loss.item():.4f}, g_loss={g_loss.item():.4f}\")\n",
        "\n",
        "\n",
        "print(f\"Final d_loss={d_loss.item():.4f}, g_loss={g_loss.item():.4f}\")\n",
        "\n",
        "# Sampling\n",
        "generator.eval()\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(8, 512, device=device)\n",
        "    samples = generator(z)\n",
        "\n",
        "samples_np = samples.cpu().numpy()\n",
        "print(\"Generated sample batch shape:\", samples_np.shape)"
      ],
      "metadata": {
        "id": "sXm_x1Whcnlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9AVipIJW6i5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xe1VUurA6i8y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}